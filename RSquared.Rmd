---
title: "RSquared"
author: "Tracy Morgan"
date: "10/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In R, we typically get R-squared by calling the summary function on a model object. Here’s a quick example using simulated data:

```{r,echo=TRUE}
# independent variable
x <- 1:20 
# for reproducibility
set.seed(1) 
# dependent variable; function of x with random error
y <- 2 + 0.5*x + rnorm(20,0,3) 
# simple linear regression
mod <- lm(y~x)
# request just the r-squared value
summary(mod)$r.squared   
```
 
 One way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations
 
$$ R^{2} =  \frac{\sum (\hat{y} – \bar{\hat{y}})^{2}}{\sum (y – \bar{y})^{2}}$$
 
 
```{r}
 # extract fitted (or predicted) values from model
f <- mod$fitted.values
# sum of squared fitted-value deviations
mss <- sum((f - mean(f))^2)
# sum of squared original-value deviations
tss <- sum((y - mean(y))^2)
# r-squared
mss/tss 
```

1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making$$σ2$$ large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.

```{r}
r2.0 <- function(sig){
  # our predictor
  x <- seq(1,10,length.out = 100)   
  # our response; a function of x plus some random noise
  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) 
  # print the R-squared value
  summary(lm(y ~ x))$r.squared          
}
sigmas <- seq(0.5,20,length.out = 20)
 # apply our function to a series of sigma values
rout <- sapply(sigmas, r2.0)            
plot(rout ~ sigmas, type="b")
```
```{r}

set.seed(1)
# our predictor is data from an exponential distribution
x <- rexp(50,rate=0.005)
# non-linear data generation
y <- (x-1)^2 * runif(50, min=0.8, max=1.2) 
# clearly non-linear 
plot(x,y) 
summary(lm(y ~ x))$r.squared

```

3. R-squared says nothing about prediction error, even with $σ2$ exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We’re better off using Mean Square Error (MSE) as a measure of prediction error.

```{r}
x <- seq(1,10,length.out = 100)
set.seed(1)
y <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)
mod1 <- lm(y ~ x)
summary(mod1)$r.squared

# Mean squared error
sum((fitted(mod1) - y)^2)/100

```
Now repeat the above code, but this time with a different range of x. Leave everything else the same:
```{r}
 # new range of x
x <- seq(1,2,length.out = 100)      
set.seed(1)
y <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)
mod1 <- lm(y ~ x)
summary(mod1)$r.squared

sum((fitted(mod1) - y)^2)/100   

```

# Shows that R squared is not reliable.  You can make it higher, we looked at 3 different ways. 

4.  R-squared can easily go down when the model assumptions are better fulfilled.

```{r}
x <- seq(1,2,length.out = 100)
set.seed(1)
y <- exp(-2 - 0.09*x + rnorm(100,0,sd = 2.5))
summary(lm(y ~ x))$r.squared

plot(lm(y ~ x), which=3)

```
R-squared is very low and our residuals vs. fitted plot reveals outliers and non-constant variance. A common fix for this is to log transform the data. Let’s try that and see what happens:

```{r}
plot(lm(log(y)~x),which = 3) 

```
The diagnostic plot looks much better. Our assumption of constant variance appears to be met. But look at the R-squared:

```{r}
summary(lm(log(y)~x))$r.squared 

```
It’s even lower! This is an extreme case and it doesn’t always happen like this. In fact, a log transformation will usually produce an increase in R-squared. But as just demonstrated, assumptions that are better fulfilled don’t always lead to higher R-squared.

5. It is very common to say that R-squared is “the fraction of variance explained” by the regression.
Yet
if we regressed X on Y, we’d get exactly the same R-squared. This in itself should be enough to show that a high R-squared says nothing about explaining one variable by another.

```{r}
x <- seq(1,10,length.out = 100)
y <- 2 + 1.2*x + rnorm(100,0,sd = 2)
summary(lm(y ~ x))$r.squared
```
```{r}
all.equal(cor(x,y)^2, summary(lm(x ~ y))$r.squared, summary(lm(y ~ x))$r.squared)

```
 
 Let’s recap:

R-squared does not measure goodness of fit.

R-squared does not measure predictive error.

R-squared does not necessarily increase when assumptions are better satisfied.

R-squared does not measure how one variable explains another.

 